{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda14579",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "NhEqBQ2hKoMX",
    "outputId": "3f2eb9c0-7eae-431d-dbb3-05cc5dc188ce",
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "#! pip install -q datasets matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f12b6f",
   "metadata": {
    "id": "7lbon_XSlnow"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# See : https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c666e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers_v = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "triton_v = \"triton==2.2.0\" if V(__version__) < V(\"2.4.0\") else \"triton\"\n",
    "triton_v, xformers_v # Versions required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd74542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HFCOMPANY=os.environ.get(\"HFCOMPANY\", \"cryptic-wordplay-formalizer\")\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "#dataset = load_dataset(\"boda/cryptonite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff246d74",
   "metadata": {
    "id": "8pmLPs_dKLpZ"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91465ccb",
   "metadata": {
    "id": "8pmLPs_dKLpZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redis server is available and running.\n"
     ]
    }
   ],
   "source": [
    "from solver import llm\n",
    "#llm_prompt_style = llm.llama3_prompt  # With tricky prompting wrapping\n",
    "llm_prompt_style = llm.alpaca_prompt\n",
    "\n",
    "#EOS_TOKEN='<|end_of_text|>'  # llama3 models\n",
    "EOS_TOKEN='<eos>'           # gemma2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ab7107",
   "metadata": {
    "id": "xhouLNG7nzLV"
   },
   "outputs": [],
   "source": [
    "def transform_to_definition_finder(wordplay_ex):\n",
    "  clue_with_def = wordplay_ex['clue']\n",
    "  return llm.prompt_definition_guesser(llm_prompt_style, clue_with_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f9dcd3-c4f3-4d71-8a3d-cec47e0c38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_wordplay_guesser(wordplay_ex):\n",
    "  clue_with_def = wordplay_ex['clue']\n",
    "  answer        = wordplay_ex['answer']\n",
    "  wordplay      = wordplay_ex['wordplay']\n",
    "  return llm.prompt_wordplay_guesses(llm_prompt_style, clue_with_def, answer, wordplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bf01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_def_and_wordplay_guesser(wordplay_ex):\n",
    "  clue_with_def = wordplay_ex['clue']\n",
    "  answer        = wordplay_ex['answer']\n",
    "  wordplay      = wordplay_ex['wordplay']\n",
    "  return llm.prompt_def_and_wordplay_guesser(llm_prompt_style, clue_with_def, answer, wordplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_def_and_wordplay_classifier(wordplay_ex):\n",
    "  clue_with_def = wordplay_ex.get('clue_with_def', wordplay_ex['clue'])   # use clue_with_def if it exists...\n",
    "  answer        = wordplay_ex['answer']\n",
    "  wordplay      = wordplay_ex['wordplay']\n",
    "  is_gold       = wordplay_ex['is_gold']\n",
    "  return llm.prompt_def_and_wordplay_classifier(llm_prompt_style, clue_with_def, answer, wordplay, is_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0780090-b463-48e8-bb85-d7a53870801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upper_case_too=False\n",
    "upper_case_too=True\n",
    "def transform_to_answer_guesser(cryptonite_ex):\n",
    "  clue_no_def = cryptonite_ex['clue'].replace('{','').replace('}','')\n",
    "  enumeration = cryptonite_ex['enumeration']\n",
    "  orientation = cryptonite_ex['orientation']\n",
    "  answer      = cryptonite_ex['answer']\n",
    "  return llm.prompt_answer_guesser(llm_prompt_style, clue_no_def, enumeration, orientation, answer,\n",
    "                                   upper_case_too=upper_case_too,\n",
    "                                   EOS_TOKEN=EOS_TOKEN) # Needed to end training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_answer_guesser_with_do_over(cryptonite_do_over_ex):\n",
    "  clue_no_def = cryptonite_do_over_ex['clue'].replace('{','').replace('}','')\n",
    "  enumeration = cryptonite_do_over_ex['enumeration']\n",
    "  orientation = cryptonite_do_over_ex['orientation']\n",
    "  answers     = cryptonite_do_over_ex['answers']\n",
    "  return llm.prompt_answer_guesser(llm_prompt_style, clue_no_def, enumeration, orientation, '', answers=answers, \n",
    "                                   EOS_TOKEN=EOS_TOKEN) # Needed to end training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c99a32",
   "metadata": {},
   "source": [
    "#### clue->def+wordplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b0ab80-9993-473f-a66f-0a766083957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver='2024-05-19'  # .json\n",
    "#ver='2024-06-25'  # Better clue bracketing .json\n",
    "ver='2024-09-23'  # Times Times-Quick and FT .jsonl (3x larger)\n",
    "\n",
    "# https://huggingface.co/docs/datasets/en/loading\n",
    "dataset_wordplay_train = load_dataset('json', data_files=f'./datasets/wordplay_{ver}_train.jsonl', split='train')\n",
    "dataset_wordplay_val   = load_dataset('json', data_files=f'./datasets/wordplay_{ver}_val.jsonl', split='train') #split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79398ef4-98a4-4cff-bcfc-276d8c2bd17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function transform_to_def_and_wordplay_guesser at 0x7f4aca93ef20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b141bc0e0c4a35a15166a9788daa50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the transformation\n",
    "#transformed_dataset = dataset_wordplay_train.map(transform_to_definition_finder)\n",
    "transformed_dataset = dataset_wordplay_train.map(transform_to_def_and_wordplay_guesser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce53b75",
   "metadata": {},
   "source": [
    "#### clue->answer for (large) Cryptonite training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_cryptonite_dataset(split): # train, val, test\n",
    "  d=[]\n",
    "  with open(f'./data_orig/cryptonite-{split}.jsonl', 'rt') as f:\n",
    "    for l in f.readlines():\n",
    "     data = json.loads(l)\n",
    "     data['number']=str(data['number'])\n",
    "     d.append(data)\n",
    "  return d\n",
    "\n",
    "dataset_cryptonite_train = Dataset.from_pandas(pd.DataFrame(load_cryptonite_dataset('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = dataset_cryptonite_train.map(transform_to_answer_guesser)\n",
    "# Takes ~ 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774de8b6",
   "metadata": {},
   "source": [
    "#### Do-over dataset (clue->answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do-over dataset here...\n",
    "dataset_do_over = Dataset.from_pandas(pd.DataFrame(load_cryptonite_dataset('do-over-llama3.1-it-v1')))\n",
    "#dataset_do_over = Dataset.from_pandas(pd.DataFrame(load_cryptonite_dataset('split='do-over-gemma2-v1'')))\n",
    "transformed_dataset = dataset_do_over.map(transform_to_answer_guesser_with_do_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0da660",
   "metadata": {},
   "source": [
    "#### proved-vs-wrong Wordplay classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54411952",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver='proved-vs-wrong_g2wordplay-val-gold_2024-10-03_18-42-36'\n",
    "dataset_wordplay_proved   = load_dataset('json', data_files=f'./datasets/wordplay_{ver}.jsonl', split='train') #split??\n",
    "transformed_dataset = dataset_wordplay_proved.map(transform_to_def_and_wordplay_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7622550",
   "metadata": {
    "id": "vKNaulW0akK7"
   },
   "source": [
    "### Check lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a23440",
   "metadata": {
    "id": "qXLCKNewLx4y"
   },
   "outputs": [],
   "source": [
    "transformed_dataset = transformed_dataset.add_column(\n",
    "  'length', [len(prompt.split(' ')) for prompt in transformed_dataset['prompt_train']]\n",
    "  # NB: This is lengths in words (split on spaces), not tokens!\n",
    ")\n",
    "transformed_dataset  # This is an HF dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277fc0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>system<|end_header_id|>\\n\\nCryptic clue wordplay generation : Given the clue and the answer, return expert definition and wordplay annotations<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nclue: \"musical and ballet, oddly, that can be avoided\"\\nanswer: EVITABLE ~ evitable<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\ndefinition: musical and ballet, oddly, {that can be avoided}\\nwordplay: EVITA (musical) + B[a]L[l]E[t] (ballet, odd letters)<|eot_id|><|end_of_text|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=401\n",
    "print(transformed_dataset[idx]['prompt_train'])\n",
    "print('***')\n",
    "print(transformed_dataset[idx]['prompt_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a15ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ExUaUpf55p2o",
    "outputId": "d9be9e46-c662-4eaf-e160-1a26f56b7b9d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(transformed_dataset)\n",
    "df['length'].hist(bins=10);  # Create a histogram of the 'length' column\n",
    "\n",
    "max_words_train = df['length'].max()\n",
    "max_words_train  # BUT THIS IS IN WORDS! (i.e. prompt is split on ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eae65a",
   "metadata": {
    "id": "vnmSZzjUlzg9"
   },
   "source": [
    "## Fine-Tune LLM using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912af018",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec130fca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-DcimwVcf5d",
    "outputId": "439f8f60-2f0b-46b6-9b7b-8d7fd50e1ed3"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "dtype = None # auto detection.\n",
    "\n",
    "model, tokenizer = None, None\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "  #model_name = \"unsloth/llama-3-8b-bnb-4bit\", # NOOOOO(?) = Base model\n",
    "  #model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Instruct version\n",
    "  #model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\", # base version of 3.1\n",
    "  #model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", # instruct version of 3.1  (BASE CASE FOR VARIATIONS)\n",
    "  #model_name = \"unsloth/gemma-2-2b-bnb-4bit\",  # NO NEED : gemma-2-2b-it-bnb-4bit\n",
    "  \n",
    "  #model_name = \"unsloth/gemma-2-9b-bnb-4bit\",  # Best base model\n",
    "\n",
    "  # Do-over training resumption\n",
    "  #model_name = \"./llama3.1-it_answer_guesser_1200_steps_resp-only\",\n",
    "\n",
    "  # wordplay classifier starting point (with loss-mask)\n",
    "  model_name =  \"./gemma2-9B_def-and-wordplay_guesser_4-epochs_resp-only\",\n",
    "  \n",
    "  # max_seq_length can be set to anything, since we do automatic RoPE Scaling via kaiokendev's method.\n",
    "  max_seq_length = 256, # May have an impact on compilation choices?\n",
    "  dtype = dtype,\n",
    "  load_in_4bit = True,\n",
    ")\n",
    "#assert EOS_TOKEN == tokenizer.eos_token\n",
    "print(EOS_TOKEN, tokenizer.eos_token)   # HMMMM  EOS_TOKEN=<|end_of_text|> for Llama3 is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=transformed_dataset[0]\n",
    "prompt_train = example['prompt_train']\n",
    "toks = tokenizer([prompt_train]) # , return_tensors=\"pt\"\n",
    "print(tokenizer.batch_decode(toks['input_ids'])[0])\n",
    "#toks # Check that <|begin_of_text|> is not repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf08e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_toks_train=None\n",
    "#max_toks_train=96 # For ->answer (slight over-estimate, of both versions)\n",
    "#max_toks_train=140 # For ->answer (do-over) (slight over-estimate, of both versions)\n",
    "#max_toks_train=228 # For ->answer (with_upper_case) (slight over-estimate, of both versions)\n",
    "\n",
    "#max_toks_train=136 # For ->def+wordplay (slight over-estimate)\n",
    "max_toks_train=136 # For def+wordplay classifier (slight over-estimate, since <train> tags will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the max length required...\n",
    "if max_toks_train is None:\n",
    "  max_toks_train = -1\n",
    "  for example in transformed_dataset:\n",
    "    prompt_train = example['prompt_train']\n",
    "    toks = tokenizer([prompt_train], return_tensors=\"pt\")\n",
    "    #print(toks['input_ids'].shape) #= torch.Size([1, 92]) # Also :: 'attention_mask'...\n",
    "    max_toks_train = max(max_toks_train, toks['input_ids'].shape[1])\n",
    "    #break\n",
    "print(max_toks_train)\n",
    "# 125 for definition_guesser (with example), 87 (no example), 93 ('expert' language)\n",
    "# 135 for def+wordplay guesser (no example)\n",
    "# 95 for answer guesses (no example) / (alpaca = 91)\n",
    "# 132 for answer guesses (with do-over training data) / (alpaca = ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b0582",
   "metadata": {
    "id": "I3oHAYYcmee8"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "  model,\n",
    "  r = 16,   # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "  #r = 32,   # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "  lora_alpha = 16,\n",
    "  lora_dropout = 0,\n",
    "  bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "  # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "  use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "  random_state = 42,\n",
    "  use_rslora = False,  # We support rank stabilized LoRA\n",
    "  loftq_config = None, # And LoftQ  use_rslora = False,\n",
    ")\n",
    "# Number of trainable parameters = 41,943,040\n",
    "# T4 GPU : 11415MiB / 15360MiB used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(transformed_dataset)>200*1000:\n",
    "  transformed_dataset = transformed_dataset.select(range(200*1000))  # Slim it down from 470k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dfc7fd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "per_device_train_batch_size, gradient_accumulation_steps = 32, 4  # worked for def+wordplay\n",
    "\n",
    "#per_device_train_batch_size, gradient_accumulation_steps = 32, 4  # worked for answer on gemma2-9B\n",
    "#per_device_train_batch_size, gradient_accumulation_steps = 64, 2  # worked for answer on Llama3\n",
    "#per_device_train_batch_size, gradient_accumulation_steps = 32, 4  # worked for answer+do-over on Llama3\n",
    "#TOO MUCH : per_device_train_batch_size, gradient_accumulation_steps = 128, 1 \n",
    "\n",
    "#per_device_train_batch_size, gradient_accumulation_steps = 16, 8  # worked for answer on gemma2-9B ... upper\n",
    "\n",
    "steps_per_epoch = len(transformed_dataset)//per_device_train_batch_size//gradient_accumulation_steps\n",
    "\n",
    "epochs, logging_steps, max_steps = 4, 10, -1 # wordplay\n",
    "#epochs, logging_steps, max_steps = 1, 25, 1200  # ->answer  # 3675 steps max (32 batch size)\n",
    "#epochs, logging_steps, max_steps = 1, 100, -1  # ->answer  # Just 1 epoch\n",
    "print(epochs, steps_per_epoch, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e87e5b-69c9-4962-8d52-d30c70d4a8f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c4e91-71ca-4a2f-805e-33b9215dfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_string_with_loss_masking(s, train_start='<train>', train_stop='</train>'):\n",
    "  idx_start, idx_stop = s.find(train_start), s.find(train_stop)\n",
    "  train_state=True\n",
    "  if idx_start>=0:\n",
    "    train_state=False\n",
    "    if idx_stop>0 and idx_stop<idx_start:\n",
    "      train_state=True\n",
    "  # Now split (retaining separators) and iterate over the pieces...\n",
    "  arr = re.split(f'({train_start}|{train_stop})', s)\n",
    "  #print('\\n', arr)\n",
    "  input_ids, attention_mask, labels = [], [], []\n",
    "  for i, segment in enumerate(arr):\n",
    "    if segment==train_start: \n",
    "      train_state=True\n",
    "      continue\n",
    "    if segment==train_stop: \n",
    "      train_state=False\n",
    "      continue\n",
    "    outputs = tokenizer(\n",
    "      segment,\n",
    "      #add_special_tokens=add_special_tokens,\n",
    "      #truncation=True,\n",
    "      #padding=False,\n",
    "      #max_length=max_seq_length,\n",
    "      #return_overflowing_tokens=False,\n",
    "      #return_length=False,\n",
    "    )\n",
    "    #print(train_state, outputs)\n",
    "    input_ids_segment = outputs[\"input_ids\"]\n",
    "    if i>0:\n",
    "      input_ids_segment = input_ids_segment[1:]\n",
    "    input_ids.extend(input_ids_segment)\n",
    "    #attention_mask.extend(outputs[\"attention_mask\"])\n",
    "    if train_state:\n",
    "      labels.extend(input_ids_segment)\n",
    "    else:\n",
    "      labels.extend([-100 for _ in input_ids_segment]) # Mask these out\n",
    "  attention_mask = [1 for _ in input_ids]\n",
    "  return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def tokenise_dataset_with_loss_masking(dataset, dataset_text_field='prompt_train'):\n",
    "  def tokenize_item_with_loss_masking(element):  # This has masking thing inside\n",
    "    return tokenize_string_with_loss_masking(element[dataset_text_field])\n",
    "  return dataset.map(tokenize_item_with_loss_masking)\n",
    "\n",
    "test_arr=[\n",
    "  'Simple test of masking',\n",
    "  'Simple test <train>of masking',\n",
    "  'Simple test</train> of masking',\n",
    "  'Simple <train>test</train> of masking',\n",
    "  'Simple <train>test</train> of <train>masking</train>',\n",
    "  '</train>Simple test of masking',\n",
    "]\n",
    "#df = pd.DataFrame(test_arr)\n",
    "#d = Dataset.from_pandas(df.rename(columns={0: \"prompt_train\"}), split=\"train\")\n",
    "#tokenise_dataset_with_loss_masking(d)[:]\n",
    "for s in test_arr:\n",
    "  print(tokenize_string_with_loss_masking(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe95e9c-70d6-4637-aef0-11329c784c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_with_loss_masking =  tokenise_dataset_with_loss_masking( transformed_dataset.select(range(3)) )\n",
    "dataset_with_loss_masking =  tokenise_dataset_with_loss_masking( transformed_dataset )\n",
    "max_toks_train, max([len(d['labels']) for d in dataset_with_loss_masking])  # Show that max_toks_train is a slight over-estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03910f23-2a47-44d5-b487-a23ada2284e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd3ca9-f0cf-4c1d-9faf-8fc319ec8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/trl/blob/v0.11.1/trl/trainer/sft_trainer.py#L510\n",
    "#   \" skip the dataset preparation by using `SFTConfig(dataset_kwargs={'skip_prepare_dataset': True})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a8353",
   "metadata": {
    "id": "JUKmCFfamoFL"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model = model,\n",
    "  tokenizer = tokenizer, \n",
    "  data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "  \n",
    "  #train_dataset = transformed_dataset,\n",
    "  #dataset_text_field = \"prompt_train\",\n",
    "\n",
    "  train_dataset = dataset_with_loss_masking,\n",
    "  dataset_kwargs={'skip_prepare_dataset': True},\n",
    "  \n",
    "  max_seq_length = max_toks_train,  # Determined above\n",
    "  dataset_num_proc = 2,\n",
    "  packing = False, # Can make training 5x faster for short sequences.\n",
    "  args = TrainingArguments(\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    warmup_steps = 5,\n",
    "    max_steps = max_steps,\n",
    "    num_train_epochs = epochs, \n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = logging_steps,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 42,\n",
    "    output_dir = \"outputs\",\n",
    "  ),\n",
    ")\n",
    "# NB: Number of examples is adjusted for packing...\n",
    "#     https://github.com/unslothai/unsloth/issues/524#issuecomment-2129192246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93378924",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # No need for this with our much fancier loss-mask function\n",
    "  from unsloth.chat_templates import train_on_responses_only\n",
    "  trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    #instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    #response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    instruction_part = \"### Instruction:\\n\",\n",
    "    response_part = \"### Response:\\n\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a41f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "print(tokenizer.decode(trainer.train_dataset[idx][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x==-100 else x for x in trainer.train_dataset[idx][\"labels\"]])\n",
    "# ValueError: expected sequence of length 59 at dim 1 (got 66) if train_on_responses_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04d694",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r30ysFJAmrrV",
    "outputId": "1c4361ab-840d-48ef-cc39-5b3e1f9eb7fb"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "# 34 steps batch_size=32, accumulation=4 - 1 epoch=20mins\n",
    "# def-word : 4 epochs = 1h46m (Llama3)\n",
    "# def-word : 4 epochs = 7hr (Gemma2) 3x larger dataset\n",
    "\n",
    "# clue->ans : 1 epoch ~ 22hrs  llama3.1 (Crashed)\n",
    "# clue->ans : 100 steps ~ 40mins (64*2 batch size) (newer version)\n",
    "# clue->ans :1200 steps ~ ~8hrs (64*2 batch size) (newer version)\n",
    "# clue->ans :1200 steps ~ 7h40min (64*2 batch size) (newer version - responses only)\n",
    "# clue->ans :1200 steps ~ 9h30min (32*4 batch size) (Gemma2 9B)\n",
    "# clue->ans :1200 steps ~ 2h50min (32*4 batch size) (Gemma2 2B)\n",
    "# clue->ans :1200 steps ~ 7h40min (64*2 batch size) (responses only - r=32)\n",
    "# clue->ans do-over : 21 steps ~ 11min (32*4 batch size) (responses only)\n",
    "# clue->ans : 1 epoch ~ 29h30min  (Gemma2 9B)\n",
    "# clue->ans :1200 steps ~ 20h20min (16*8 batch size) (Gemma2 9B upper)\n",
    "\"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb00cc1",
   "metadata": {
    "id": "bIZ8CweBboW1",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 epoch losses (with 1 example)\n",
    " 5\t4.541300\n",
    "10\t2.439500\n",
    "15\t1.087100\n",
    "20\t0.795400\n",
    "25\t0.730300\n",
    "30\t0.691300\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "3 epoch losses (with 1 example)\n",
    " 5\t4.541300\n",
    "10\t2.425500\n",
    "15\t1.044400\n",
    "20\t0.762900\n",
    "75\t0.580600\n",
    "80\t0.589000\n",
    "85\t0.586200\n",
    "90\t0.570500\n",
    "95\t0.562800\n",
    "100\t0.560300\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "3 epoch losses (with no example)\n",
    " 5\t5.596100\n",
    "10\t3.071500\n",
    "15\t1.509300\n",
    "20\t1.174000\n",
    "75\t0.952600\n",
    "80\t0.946200\n",
    "85\t0.969400\n",
    "90\t0.941100\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def+wordplay 4 epochs\n",
    "  5\t5.539400\n",
    " 10\t2.427100\n",
    " 15\t1.261500\n",
    " 20\t1.086400\n",
    "150\t0.662300\n",
    "155\t0.665200\n",
    "160\t0.671900\n",
    "165\t0.673900\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def+wordplay 4 epochs (Gemma2-9B, 2024-09-23)\n",
    "Step\tTraining Loss\n",
    "10\t1.352600\n",
    "20\t0.748000\n",
    "40\t0.631300\n",
    "80\t0.527800\n",
    "100\t0.517200\n",
    "200\t0.380700\n",
    "300\t0.272000\n",
    "400\t0.211500\n",
    "500\t0.140300\n",
    "520\t0.141400\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "->ans 100 steps (SFT on full prompt)\n",
    " Step\tTraining Loss\n",
    "25\t2.285200\n",
    "50\t0.894200\n",
    "75\t0.870800\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on full prompt)\n",
    "Step\tTraining Loss\n",
    "25\t2.288000\n",
    "50\t0.898100\n",
    "75\t0.884900\n",
    "100\t0.874800\n",
    "500\t0.835200\n",
    "800\t0.821500\n",
    "1000\t0.811700\n",
    "1200\t0.811400\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only) BASE-CASE\n",
    "Step\tTraining Loss\n",
    "25\t2.210400\n",
    "50\t0.964600\n",
    "75\t0.902800\n",
    "100\t0.875900\n",
    "200\t0.815700\n",
    "400\t0.735800\n",
    "800\t0.653400\n",
    "1000\t0.630100\n",
    "1200\t0.619800\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only, with 'expert' prompt)\n",
    "Step\tTraining Loss\n",
    "25\t2.237600\n",
    "50\t0.964000\n",
    "75\t0.904700\n",
    "100\t0.879700\n",
    "200\t0.812200\n",
    "400\t0.739200\n",
    "800\t0.654700\n",
    "1000\t0.631500\n",
    "1200\t0.622000\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only, llama 3 -it)\n",
    "Step\tTraining Loss\n",
    "25\t2.770800\n",
    "50\t0.965900\n",
    "75\t0.909000\n",
    "100\t0.890600\n",
    "200\t0.823400\n",
    "400\t0.742600\n",
    "800\t0.656400\n",
    "1000\t0.635100\n",
    "1200\t0.622900\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only, gemma2 9B base - alpaca)\n",
    "Step\tTraining Loss\n",
    "25\t1.142900\n",
    "50\t0.899000\n",
    "75\t0.857300\n",
    "100\t0.838800\n",
    "200\t0.784900\n",
    "400\t0.699200\n",
    "800\t0.623100\n",
    "1000\t0.587900\n",
    "1200\t0.579900\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only, gemma2 2B base - alpaca)\n",
    "Step\tTraining Loss\n",
    "25\t1.480600\n",
    "50\t1.194000\n",
    "75\t1.145800\n",
    "100\t1.122400\n",
    "200\t1.045000\n",
    "400\t0.944200\n",
    "800\t0.853300\n",
    "1000\t0.822700\n",
    "1200\t0.818500\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only, llama3.1 7B base - alpaca)\n",
    "Step\tTraining Loss\n",
    "25\t1.171500\n",
    "50\t0.931700\n",
    "75\t0.891300\n",
    "100\t0.866800\n",
    "200\t0.804500\n",
    "400\t0.730300\n",
    "800\t0.645600\n",
    "1000\t0.628500\n",
    "1200\t0.619400\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only) r=32 instead of r=16\n",
    "Step\tTraining Loss\n",
    "25\t2.212800\n",
    "50\t0.966800\n",
    "75\t0.903900\n",
    "100\t0.880700\n",
    "200\t0.818300\n",
    "400\t0.744000\n",
    "800\t0.656300\n",
    "1000\t0.632800\n",
    "1200\t0.624100\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only) BASE-CASE + do-over-v1\n",
    "Step\tTraining Loss\n",
    "5\t2.141500\n",
    "10\t0.718400\n",
    "15\t0.591400\n",
    "20\t0.580400\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "->ans 1200 steps (SFT on resp only, gemma2 9B base - alpaca) UPPER tokeniser trick\n",
    "Step\tTraining Loss\n",
    "25\t0.752500\n",
    "50\t0.623600\n",
    "75\t0.588400\n",
    "100\t0.575300\n",
    "200\t0.533200\n",
    "400\t0.475500\n",
    "800\t0.414300\n",
    "1000\t0.398200\n",
    "1200\t0.381800\n",
    "\"\"\";\n",
    "\"\"\"\n",
    "wordplay->classification 14 epochs (SFT loss-mask, gemma2 9B def+wordplay as starter)\n",
    "Step\tTraining Loss\n",
    "10\t2.155000\n",
    "20\t0.409100\n",
    "30\t0.159600\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ffa67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "MTnmou5ZUOkH",
    "outputId": "9d0135c2-83c2-4e91-a372-032fc8f1e8a4"
   },
   "outputs": [],
   "source": [
    "prompt_test = transformed_dataset[0]['prompt_test']\n",
    "prompt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969696a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZVpYIFrPHLJ",
    "outputId": "96fd042b-6048-461a-97f2-4b5834eab67c"
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "FastLanguageModel.for_inference(model); # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071f358",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZVpYIFrPHLJ",
    "outputId": "96fd042b-6048-461a-97f2-4b5834eab67c"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_test], return_tensors='pt').to(\"cuda\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens = 32,\n",
    "                         use_cache = True,)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset[0]['answer'].upper()  # Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b5b84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92toF5tnR7KC",
    "outputId": "4b794116-1719-4bea-b1cd-ae31a15fff3f"
   },
   "outputs": [],
   "source": [
    "# local save (in current path!)\n",
    "#model_path = \"./llama3-it_def_and_wordplay_guesser_4_epoch_noex\"\n",
    "\n",
    "# BASE-CASE Version\n",
    "#model_path = \"./llama3.1-it_answer_guesser_1200_steps_resp-only\"\n",
    "\n",
    "# Updated text, with system prompt including 'expert'\n",
    "#model_path = \"./llama3.1-it_answer_guesser_1200_steps_resp-only_expert\"\n",
    "\n",
    "# Reverted text, with regular system prompt on *Llama 3 it*\n",
    "#model_path = \"./llama3-it_answer_guesser_1200_steps_resp-only\"\n",
    "\n",
    "# Alpaca prompt for Gemma2-9B-base\n",
    "#model_path = \"./gemma2-9B_answer_guesser_1200_steps_resp-only\"\n",
    "\n",
    "# Alpaca prompt for Gemma2-9B-base 1-epoch << Base ICLR clue->answer\n",
    "#model_path = \"./gemma2-9B_answer_guesser_3678_steps_resp-only\"\n",
    "\n",
    "# Alpaca prompt for Gemma2-2B-base\n",
    "#model_path = \"./gemma2-2B_answer_guesser_1200_steps_resp-only\"\n",
    "\n",
    "# Alpaca prompt for *Llama 3.1 base*\n",
    "#model_path = \"./llama3.1-base_answer_guesser_1200_steps_resp-only\"\n",
    "\n",
    "# Base-case, but with r=32 instead of r=16 for LoRA\n",
    "#model_path = \"./llama3.1-it_answer_guesser_1200_steps_resp-only_r32\"\n",
    "\n",
    "# BASE-CASE Version + 2733 do-over-v1\n",
    "#model_path = \"./llama3.1-it_answer_guesser_1200_steps_resp-only_do-over-v1\"\n",
    "\n",
    "# Alpaca prompt for Gemma2-9B-base 1-epoch with ' U P P E R ' tokenisation idea\n",
    "#model_path = \"./gemma2-9B_answer_guesser_1200_steps_resp-only_upper\"\n",
    "\n",
    "# Alpaca prompt for Gemma2-9B-base 4-epoch for new def+wordplay dataset\n",
    "#model_path = \"./gemma2-9B_def-and-wordplay_guesser_4-epochs_resp-only\"\n",
    "\n",
    "# Alpaca prompt for Gemma2-9B-base 1-epoch of def+wordplay classifier (backend by Python provability)\n",
    "model_path = \"./gemma2-9B_def-and-wordplay_classifier_4-epochs_loss-mask\"\n",
    "\n",
    "model.save_pretrained(model_path)  \n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2905f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "b8279be3f42444618fd8e6976787da1c",
      "8844af5395ec46e5b5be9449b61c30f5",
      "aaafb685d5144444b301635e07a93010",
      "3bf21364481244eabce48b8ae1dc99ab",
      "6ad2bbbbcc2b4a0282131ca110a9ed64",
      "fce0623965854b2299f06a2b8eda5a0e",
      "424db239a19941f782ab5c823172a2fa",
      "627bdefb27c94bc786a89fa905d4108a",
      "d775713e8c9c4dc2a79e3747819d4296",
      "3f508e1da70f40259c175fbedc227752",
      "aa8cbdf60172422289a7119e1f5982ab",
      "6ef2f7b70e7d44f49cd5bc4efa66aa74",
      "3cc10acbb7ba472fbd3d1e328845ea0c",
      "1bb75d363b0b4263b45d124781e7a3ac",
      "6848fe35778f448e9994812f8fe279c1",
      "db281bb0cc214af6b947694f0f4bc6b1",
      "35b0a290153f40558e2a2c3aee3ab300",
      "71afedc56f0d420ebba6a1bf11ec012d",
      "b093bdf05c424c62b2cefe1496d9c455",
      "4eb07278e0cf41cb87935df269ba53e5",
      "88ea09ec088941e3b47161f199442f27",
      "4a6404012e564a848f60cb15ea695fcf"
     ]
    },
    "id": "uQSPOX-vSa4k",
    "outputId": "61d101f1-366e-42d7-ccb3-238b958eaa96"
   },
   "outputs": [],
   "source": [
    "# HF save\n",
    "#model.push_to_hub(f\"{HFCOMPANY}/llama3-it_definition_guesser_1_epoch_2024-06-22\", private=True)\n",
    "#tokenizer.push_to_hub(f\"{HFCOMPANY}/llama3-it_definition_guesser_1_epoch_2024-06-22\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "->ans 1-epoch=3678 steps (SFT on resp only, gemma2 9B base - alpaca)\n",
    "Step\tTraining Loss\n",
    "100\t0.928800\n",
    "200\t0.790400\n",
    "300\t0.746700\n",
    "400\t0.713500\n",
    "500\t0.687700\n",
    "600\t0.652100\n",
    "700\t0.636500\n",
    "800\t0.616900\n",
    "900\t0.600600\n",
    "1000\t0.585000\n",
    "1100\t0.574100\n",
    "1200\t0.557200\n",
    "1300\t0.554200\n",
    "1400\t0.538600\n",
    "1500\t0.522100\n",
    "1600\t0.513300\n",
    "1700\t0.503100\n",
    "1800\t0.495700\n",
    "1900\t0.482000\n",
    "2000\t0.467400\n",
    "2100\t0.463800\n",
    "2200\t0.464200\n",
    "2300\t0.450000\n",
    "2400\t0.448700\n",
    "2500\t0.440500\n",
    "2600\t0.428600\n",
    "2700\t0.414800\n",
    "2800\t0.414100\n",
    "2900\t0.406600\n",
    "3000\t0.407600\n",
    "3100\t0.396600\n",
    "3200\t0.396300\n",
    "3300\t0.387400\n",
    "3400\t0.394500\n",
    "3500\t0.385800\n",
    "3600\t0.381600\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20ae88",
   "metadata": {
    "id": "GNtbjP6NX1XA"
   },
   "source": [
    "## Loading the inference from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc1f350",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "-se1bSENbGKX",
    "outputId": "a0501fe1-48d7-4c42-e02d-0161d1987de9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt_test \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_dataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m prompt_test\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformed_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_test = transformed_dataset['train'][3]['prompt_test']\n",
    "prompt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc24b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639,
     "referenced_widgets": [
      "cd3a1577146f4ea497683d5e7dff8de3",
      "46a111c44bfc47508a25280ea3c8c8b3",
      "86d34683793245a7bf079b7fea06a0e6",
      "728b866b1120480dab10b4ecebfa4126",
      "a68ef6877b2749e9bc8a9ec3b7bdbbae",
      "4f0824159ea640c7983c8f94145ee0f1",
      "3c0ef7a84c1f42ce8a631330f630aa89",
      "4f0fbf8195a44e97848abf58c506f1f8",
      "e8af42cf988f4835aef11c3fd5eb9b32",
      "022d25ccaa5747f0bbbce7c870f47832",
      "3b3c298fe16a4374b67c07e4155b6fce",
      "79d49f1f637b47fc95cae7d703a79a51",
      "01037ef6fdf04744988e98112b717a87",
      "65494ee520164cd98cb707e2c25a543c",
      "1571ff5a2c364d1fb6b71df909ca95ec",
      "c99fa5fe92674c69ac90f5af8eca77d7",
      "5450afdaf8054406b40c2dbe8a6e519b",
      "3deebe51ae964bbeb872dec6fbd5debe",
      "3cff437169ae4a59a9408c1a23be9af5",
      "afb93b4b65754918b0d2429af144bd81",
      "20e5370608bf410a98bc733b41de6bc9",
      "3dd29bc2f4b94c8284ee8c6828d405d6"
     ]
    },
    "editable": true,
    "id": "U0A7p9eWTEtl",
    "outputId": "05fa8098-00c9-4be0-a7bc-2ef404df3e82",
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = f\"{HFCOMPANY}/llama3_cryptonite_test_100_steps\",\n",
    "    max_seq_length = max_toks_train,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Faster inference\n",
    "\n",
    "inputs = tokenizer([prompt_test], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bde83",
   "metadata": {
    "id": "QFBnA65MDonu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7450c4",
   "metadata": {},
   "source": [
    "## Test a raw (new) prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clue: \"stop fighting that butchery he spread\"\n",
    "definition: {stop fighting} that butchery he spread\n",
    "wordplay: (THAT BUTCHERY HE)* (*spread)\n",
    "answer: BURY THE HATCHET ~ bury the hatchet\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "clue: \"professional body is a costly one we fancy\"\n",
    "definition: {professional body} is a costly one we fancy\n",
    "# Not right, really\n",
    "wordplay: (IS A COSTLY)* (*one we fancy = anagram)\n",
    "# Better\n",
    "wordplay: (A COSTLY + I + WE)* (*fancy)  \n",
    "answer: LAW SOCIETY ~ law society\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "clue: \"create difficulty for philosopher endlessly cut by left\"\n",
    "definition: {create difficulty for} philosopher endlessly cut by left\n",
    "wordplay: HOBBE[s] = philosopher endlessly, cut by L = left\n",
    "answer: HOBBLE ~ hobble\n",
    "definition: create difficulty for philosopher {endlessly cut by left}\n",
    "wordplay: create difficulty for (MA, RAGE, IN) philosopher (not endless – cut by left)\n",
    "answer: MARGIN ~ margin\n",
    "\"\"\"\n",
    "\n",
    "prompt_test='''### Instruction:\\nCryptic clue wordplay verification : For the given clue, expertly classify whether the suggested definition, wordplay and answer is valid (True/False)\n",
    "\n",
    "### Input:\n",
    "clue: \"create difficulty for philosopher endlessly cut by left\"\n",
    "definition: {create difficulty for} philosopher endlessly cut by left\n",
    "wordplay: HOBBE[s] = philosopher endlessly, cut by L = left\n",
    "answer: HOBBLE ~ hobble\n",
    "\n",
    "### Response:\n",
    "is_valid:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_test], return_tensors='pt').to(\"cuda\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens = 32,\n",
    "                         use_cache = True,)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ae76e-919a-4225-90f6-bb0ff51b7317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "cache-notebooks//ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
